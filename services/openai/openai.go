package openai

import (
	"context"
	"fmt"
	"math"
	"strings"
	"time"

	"github.com/modernice/jotbot/generate"
	"github.com/modernice/jotbot/internal"
	"github.com/sashabaranov/go-openai"
	"github.com/tiktoken-go/tokenizer"
	"golang.org/x/exp/slog"
)

const (
	// DefaultModel represents the pre-configured model identifier to be used by the
	// Service when no specific model is provided. This default serves as a fallback
	// and ensures that the Service has a valid starting point for its operations
	// related to AI-powered text generation or completion tasks.
	DefaultModel = openai.GPT3Dot5Turbo

	// DefaultMaxTokens represents the initial setting for the maximum number of
	// tokens that can be generated by the service. This value is used as a default
	// when no specific maximum is set through service configuration options. It
	// ensures that the output from token generation does not exceed a predefined
	// length, providing a balance between performance and output detail.
	DefaultMaxTokens = 512
)

// Service orchestrates the generation of textual content using a specified
// model from OpenAI. It manages the creation of requests, execution of the
// underlying model, and interpretation of the results to produce coherent
// output. Service provides options for customizing the model, client
// configuration, logging, and token limits. It supports both standard and
// chat-based completions, adapting to the input prompts provided by users
// through a defined context. The Service ensures that the generated content
// adheres to specified constraints such as token limits, while also handling
// error scenarios and logging usage information.
type Service struct {
	client    *openai.Client
	model     string
	maxTokens int
	codec     tokenizer.Codec
	log       *slog.Logger
}

// Option represents a configuration setting that can be applied to customize
// the behavior of a Service. It acts as a modifier for a Service instance,
// allowing specific aspects such as the model used, the client configuration,
// maximum token count, or logging details to be set according to the user's
// preferences. Options are designed to be passed to the Service constructor,
// enabling a flexible and fluent API for Service configuration.
type Option func(*Service)

// Model configures the model to be used by the Service for generating content.
// It accepts a model identifier as a parameter and returns an Option that, when
// applied, sets the model field in the Service struct to the specified
// identifier.
func Model(model string) Option {
	return func(s *Service) {
		s.model = model
	}
}

// Client configures a Service with the provided OpenAI client instance. It is
// used as an option when creating a new Service. This allows for the use of a
// custom OpenAI client instead of the default one that would be created using
// an API key.
func Client(c *openai.Client) Option {
	return func(s *Service) {
		s.client = c
	}
}

// MaxTokens sets the maximum number of tokens to use for generating content
// with the service. It configures the service instance by applying a limit on
// the token count for output generation, which can affect the verbosity and
// detail of the generated text. The provided maximum should not exceed the
// model's inherent token limit. If it does, the service will automatically
// adjust to use the maximum allowed by the model or the remaining tokens after
// accounting for input prompt tokens, whichever is lower. This option helps in
// managing resource usage and controlling the length of generated content.
func MaxTokens(max int) Option {
	return func(s *Service) {
		s.maxTokens = max
	}
}

// WithLogger configures a logging handler for the service, allowing the service
// to log its activities. It accepts a logging handler and returns an option
// that can be passed to the service constructor.
func WithLogger(h slog.Handler) Option {
	return func(s *Service) {
		s.log = slog.New(h)
	}
}

// New initializes a new instance of Service with the provided API key and
// options, returning a pointer to the service and any error encountered during
// the setup. It applies the given options to customize the Service, such as
// setting a specific model, client, maximum tokens, or logger. If no client is
// provided, it creates a new client using the API key. If no model is
// specified, it defaults to the predefined model. The function also ensures
// that an appropriate tokenizer and a non-nil logger are set up for the service
// before returning.
func New(apiKey string, opts ...Option) (*Service, error) {
	svc := Service{maxTokens: DefaultMaxTokens}
	for _, opt := range opts {
		opt(&svc)
	}
	if svc.client == nil {
		svc.client = openai.NewClient(apiKey)
	}

	if svc.model == "" {
		svc.log.Debug(fmt.Sprintf("[OpenAI] No model provided. Using default model %q", DefaultModel))
		svc.model = DefaultModel
	}
	svc.log.Debug(fmt.Sprintf("[OpenAI] Using model %q", svc.model))

	codec, err := internal.OpenAITokenizer(svc.model)
	if err != nil {
		return nil, fmt.Errorf("get tokenizer for model %q: %w", svc.model, err)
	}
	svc.codec = codec

	if svc.log == nil {
		svc.log = internal.NopLogger()
	}

	return &svc, nil
}

// GenerateDoc creates a document based on the context provided by the caller.
// It logs the generation process, constructs a request tailored to the input
// context, and invokes the appropriate model to generate content. The function
// returns the generated text or an error if the generation process fails. The
// operation respects a timeout and ensures that the size of the generated
// content does not exceed predefined token limits.
func (svc *Service) GenerateDoc(ctx generate.Context) (string, error) {
	svc.log.Debug(fmt.Sprintf("[OpenAI] Generating docs for %s (%s)", ctx.Input().Identifier, ctx.Input().Language))

	req := svc.makeBaseRequest(ctx)

	generate := svc.useModel(req.Model)

	timeout, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	result, err := generate(timeout, req)
	if err != nil {
		return "", err
	}
	result.normalize()

	return result.text, nil
}

func (svc *Service) makeBaseRequest(ctx generate.Context) openai.CompletionRequest {
	req := openai.CompletionRequest{
		Model:            string(svc.model),
		Temperature:      0.618,
		TopP:             0.3,
		PresencePenalty:  0.2,
		FrequencyPenalty: 0.3,
		Prompt:           ctx.Prompt(),
	}

	return req
}

func (svc *Service) useModel(model string) func(context.Context, openai.CompletionRequest) (result, error) {
	if isChatModel(model) {
		return svc.createWithChat
	}
	return svc.createWithGPT
}

func (svc *Service) createWithGPT(ctx context.Context, req openai.CompletionRequest) (result, error) {
	maxTokens, err := svc.maxGPTTokens(req.Prompt.(string))
	if err != nil {
		return result{}, fmt.Errorf("max tokens: %w", err)
	}
	req.MaxTokens = maxTokens

	resp, err := svc.client.CreateCompletion(ctx, req)
	if err != nil {
		return result{}, err
	}

	if len(resp.Choices) == 0 {
		return result{}, fmt.Errorf("openai: no choices returned")
	}

	svc.printUsage(resp.Usage)

	choice := resp.Choices[0]

	return result{
		finishReason: choice.FinishReason,
		text:         choice.Text,
	}, nil
}

func (svc *Service) createWithChat(ctx context.Context, req openai.CompletionRequest) (result, error) {
	messages := []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleUser,
			Content: req.Prompt.(string),
		},
	}

	maxTokens, err := svc.maxChatTokens(messages)
	if err != nil {
		return result{}, fmt.Errorf("max tokens: %w", err)
	}

	resp, err := svc.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model:            req.Model,
		Temperature:      req.Temperature,
		MaxTokens:        maxTokens,
		PresencePenalty:  req.PresencePenalty,
		FrequencyPenalty: req.FrequencyPenalty,
		Messages:         messages,
	})
	if err != nil {
		return result{}, err
	}

	svc.printUsage(resp.Usage)

	choice := resp.Choices[0]
	res := result{
		finishReason: string(choice.FinishReason),
		text:         choice.Message.Content,
	}

	if choice.Message.Role != openai.ChatMessageRoleAssistant {
		return res, fmt.Errorf("openai: unexpected message role in answer: %q", choice.Message.Role)
	}

	return res, nil
}

func (svc *Service) maxGPTTokens(prompt string) (int, error) {
	promptTokens, err := PromptTokens(svc.model, prompt)
	if err != nil {
		return 0, fmt.Errorf("compute tokens for prompt: %w", err)
	}

	maxTokensForModel, ok := modelMaxTokens[string(svc.model)]
	if !ok {
		maxTokensForModel = modelMaxTokens["default"]
	}

	remaining := maxTokensForModel - promptTokens

	maxTokens := int(math.Min(float64(svc.maxTokens), float64(maxTokensForModel)))
	maxTokens = int(math.Min(float64(maxTokens), float64(remaining)))
	if maxTokens < 0 {
		maxTokens = 0
	}

	return maxTokens, nil
}

func (svc *Service) maxChatTokens(messages []openai.ChatCompletionMessage) (int, error) {
	promptTokens, err := ChatTokens(svc.model, messages)
	if err != nil {
		return 0, fmt.Errorf("compute tokens for chat messages: %w", err)
	}

	maxTokensForModel, ok := modelMaxTokens[string(svc.model)]
	if !ok {
		maxTokensForModel = modelMaxTokens["default"]
	}

	remaining := maxTokensForModel - promptTokens

	maxTokens := int(math.Min(float64(svc.maxTokens), float64(maxTokensForModel)))
	maxTokens = int(math.Min(float64(maxTokens), float64(remaining)))
	if maxTokens < 0 {
		maxTokens = 0
	}

	return maxTokens, nil
}

func (svc *Service) printUsage(usage openai.Usage) {
	svc.log.Debug("[OpenAI] Usage info", "prompt", usage.PromptTokens, "completion", usage.CompletionTokens, "total", usage.TotalTokens)
}

func isChatModel(model string) bool {
	return strings.HasPrefix(model, "gpt-")
}

// MaxTokensForModel retrieves the maximum number of tokens allowed for a given
// model. If the specified model is not found in the predefined list, it returns
// the default maximum token count. This limit is crucial for ensuring that
// token generation stays within the bounds set by the model's capabilities.
func MaxTokensForModel(model string) int {
	if t, ok := modelMaxTokens[model]; ok {
		return t
	}
	return modelMaxTokens["default"]
}

var modelMaxTokens = map[string]int{
	"default":                2049,
	openai.GPT4TurboPreview:  128000,
	openai.GPT4VisionPreview: 128000,
	openai.GPT432K0314:       32768,
	openai.GPT432K:           32768,
	openai.GPT40314:          8192,
	openai.GPT4:              8192,
	openai.GPT3Dot5Turbo16K:  16384,
	openai.GPT3Dot5Turbo0301: 4096,
	openai.GPT3Dot5Turbo:     4096,
}

type result struct {
	finishReason string
	text         string
}

func (r *result) normalize() {
	r.text = strings.TrimSpace(r.text)
}
